{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 - Dario Placencio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Language Identification with Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use files 0.txt to 9.txt in each language as the training data.\n",
    "Estimate the prior probabilities \n",
    "$\\hat p(y=e)$,\n",
    "$\\hat p(y=j)$,\n",
    "$\\hat p(y=s)$\n",
    "using additive smoothing with parameter $\\frac{1}{2}$. \n",
    "Give the formula for additive smoothing with parameter $\\frac{1}{2}$ in this case. \n",
    "Print and include in final report the prior probabilities.\n",
    "(Hint: Store all probabilities here and below in $\\log()$ internally to avoid underflow. This also means you need to do arithmetic in log-space.  But answer questions with probability, not log probability.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the extract_path to point directly to the languageID folder\n",
    "extract_path = 'languageID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15339, 16500, 15496)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the contents of the training files\n",
    "def read_files(lang, start, end):\n",
    "    \"\"\"Read files for a given language between start and end indices.\"\"\"\n",
    "    contents = \"\"\n",
    "    for i in range(start, end + 1):\n",
    "        with open(os.path.join(extract_path, f\"{lang}{i}.txt\"), 'r', errors='ignore') as f:\n",
    "            contents += f.read()\n",
    "    return contents\n",
    "\n",
    "# Extract training data for each language\n",
    "english_train = read_files('e', 0, 9)\n",
    "spanish_train = read_files('s', 0, 9)\n",
    "japanese_train = read_files('j', 0, 9)\n",
    "\n",
    "len(english_train), len(spanish_train), len(japanese_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given data and smoothing parameters\n",
    "N_y = 10  # Number of English training documents\n",
    "N = 30   # Total number of training documents\n",
    "alpha = 0.5  # Smoothing parameter\n",
    "V = 3  # Number of classes (languages)\n",
    "\n",
    "# Compute smoothed prior probability for English\n",
    "p_e_smoothed = (N_y + alpha) / (N + alpha * V)\n",
    "p_e_smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store as log probabilities\n",
    "p_e_smoothed = np.log(p_e_smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute smoothed prior probability for Spanish\n",
    "p_s_smoothed = (N_y + alpha) / (N + alpha * V)\n",
    "p_s_smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store as log probabilities\n",
    "p_s_smoothed = np.log(p_s_smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute smoothed prior probability for Japanese\n",
    "p_j_smoothed = (N_y + alpha) / (N + alpha * V)\n",
    "p_j_smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store as log probabilities\n",
    "p_j_smoothed = np.log(p_j_smoothed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same training data, estimate the class conditional probability (multinomial parameter) for English $\\theta_{i,e} := \\hat p(c_i \\mid y=e)$ where $c_i$ is the $i$-th character. That is, $c_1 = a, \\ldots, c_{26} = z, c_{27} = space$. \n",
    "\n",
    "Again use additive smoothing with parameter $\\frac{1}{2}$. Give the formula for additive smoothing with parameter $\\frac{1}{2}$ in this case. \n",
    "\n",
    "Print $\\theta_e$ and include in final report which is a vector with 27 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0.0601685114819098, 'b': 0.011134974392863043, 'c': 0.021509995043779945, 'd': 0.021972575582355856, 'e': 0.1053692383941847, 'f': 0.018932760614571286, 'g': 0.017478936064761277, 'h': 0.047216256401784236, 'i': 0.055410540227986124, 'j': 0.001420783082768875, 'k': 0.0037336857756484387, 'l': 0.028977366595076822, 'm': 0.020518751032545846, 'n': 0.057921691723112505, 'o': 0.06446390219725756, 'p': 0.01675202378985627, 'q': 0.0005617049396993227, 'r': 0.053824549810011564, 's': 0.06618205848339666, 't': 0.08012555757475633, 'u': 0.026664463902197257, 'v': 0.009284652238559392, 'w': 0.015496448042293078, 'x': 0.001156451346439782, 'y': 0.013844374690236246, 'z': 0.0006277878737815959, ' ': 0.1792499586981662}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Function to count the frequency of each character in a given text\n",
    "def char_frequencies(text):\n",
    "    freq = defaultdict(int)\n",
    "    for char in text:\n",
    "        # Only consider lowercase alphabets and space\n",
    "        if char in 'abcdefghijklmnopqrstuvwxyz ':\n",
    "            freq[char] += 1\n",
    "    return freq\n",
    "\n",
    "# Assuming english_train contains the combined content of English training files\n",
    "english_freq = char_frequencies(english_train)\n",
    "\n",
    "# Compute the total number of characters in the English training data\n",
    "total_english_chars = sum(english_freq.values())\n",
    "\n",
    "# Vocabulary\n",
    "vocabulary = 'abcdefghijklmnopqrstuvwxyz '\n",
    "\n",
    "# Smoothing parameter\n",
    "alpha = 0.5\n",
    "\n",
    "# Compute the class conditional probabilities for English\n",
    "theta_e = {}\n",
    "for char in vocabulary:\n",
    "    theta_e[char] = (english_freq[char] + alpha) / (total_english_chars + alpha * len(vocabulary))\n",
    "\n",
    "print(theta_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store as log probabilities\n",
    "for char in vocabulary:\n",
    "    theta_e[char] = np.log(theta_e[char])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print $\\theta_j, \\theta_s$ and include in final report the class conditional probabilities for Japanese and Spanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta_j: {'a': 0.1317656102589189, 'b': 0.010866906600510151, 'c': 0.005485866033054963, 'd': 0.01722631818022992, 'e': 0.06020475907613823, 'f': 0.003878542227191726, 'g': 0.014011670568503443, 'h': 0.03176211607673224, 'i': 0.09703343932352633, 'j': 0.0023411020650616725, 'k': 0.05740941332681086, 'l': 0.001432614696530277, 'm': 0.03979873510604843, 'n': 0.05671057688947902, 'o': 0.09116321324993885, 'p': 0.0008735455466648031, 'q': 0.00010482546559977637, 'r': 0.04280373178657535, 's': 0.0421747789929767, 't': 0.056990111464411755, 'u': 0.07061742199238269, 'v': 0.0002445927530661449, 'w': 0.01974212935462455, 'x': 3.4941821866592126e-05, 'y': 0.01415143785596981, 'z': 0.00772214263251686, ' ': 0.12344945665466997}\n",
      "Theta_s: {'a': 0.10456045141993771, 'b': 0.008232863618143134, 'c': 0.03752582405722919, 'd': 0.039745922111559924, 'e': 0.1138108599796491, 'f': 0.00860287996053159, 'g': 0.0071844839813758445, 'h': 0.0045327001942585795, 'i': 0.049859702136844375, 'j': 0.006629459467793161, 'k': 0.0002775122567913416, 'l': 0.052943171656748174, 'm': 0.02580863988159477, 'n': 0.054176559464709693, 'o': 0.07249236841293824, 'p': 0.02426690512164287, 'q': 0.007677839104560451, 'r': 0.05929511886774999, 's': 0.06577040485954797, 't': 0.03561407295488884, 'u': 0.03370232185254849, 'v': 0.00588942678301625, 'w': 9.250408559711388e-05, 'x': 0.0024976103111220747, 'y': 0.007862847275754679, 'z': 0.0026826184823163022, ' ': 0.16826493170115014}\n"
     ]
    }
   ],
   "source": [
    "# Compute character frequencies for Japanese and Spanish\n",
    "japanese_freq = char_frequencies(japanese_train)\n",
    "spanish_freq = char_frequencies(spanish_train)\n",
    "\n",
    "# Compute the total number of characters in the Japanese and Spanish training data\n",
    "total_japanese_chars = sum(japanese_freq.values())\n",
    "total_spanish_chars = sum(spanish_freq.values())\n",
    "\n",
    "# Compute the class conditional probabilities for Japanese\n",
    "theta_j = {}\n",
    "for char in vocabulary:\n",
    "    theta_j[char] = (japanese_freq[char] + alpha) / (total_japanese_chars + alpha * len(vocabulary))\n",
    "\n",
    "# Compute the class conditional probabilities for Spanish\n",
    "theta_s = {}\n",
    "for char in vocabulary:\n",
    "    theta_s[char] = (spanish_freq[char] + alpha) / (total_spanish_chars + alpha * len(vocabulary))\n",
    "\n",
    "print(\"Theta_j:\", theta_j)\n",
    "print(\"Theta_s:\", theta_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store as log probabilities\n",
    "for char in vocabulary:\n",
    "    theta_j[char] = np.log(theta_j[char])\n",
    "    theta_s[char] = np.log(theta_s[char])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treat e10.txt as a test document $x$. Represent $x$ as a bag-of-words count vector (Hint: the vocabulary has size 27).\n",
    "\n",
    "Print the bag-of-words vector $x$ and include in final report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words Vector: [164, 32, 53, 57, 311, 55, 51, 140, 140, 3, 6, 85, 64, 139, 182, 53, 3, 141, 186, 225, 65, 31, 47, 4, 38, 2, 498]\n"
     ]
    }
   ],
   "source": [
    "# Read the content of e10.txt\n",
    "with open('languageID/e10.txt', 'r', errors='ignore') as f:\n",
    "    test_content = f.read()\n",
    "\n",
    "# Compute character frequencies for the test content\n",
    "test_freq = char_frequencies(test_content)\n",
    "\n",
    "# Create the bag-of-words vector\n",
    "bow_vector = [test_freq[char] for char in vocabulary]\n",
    "\n",
    "print(\"Bag-of-Words Vector:\", bow_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute $\\hat p(x \\mid y)$ for $y=e, j, s$ under the multinomial model assumption, respectively. Use the formula\n",
    "$\\hat p(x \\mid y) = \\prod_{i=1}^d \\theta_{i, y}^{x_i}$ where $x=(x_1, \\ldots, x_d)$.\n",
    "\n",
    "Show the three values: $\\hat p(x \\mid y=e), \\hat p(x \\mid y=j), \\hat p(x \\mid y=s)$.\n",
    "\n",
    "Hint: you may notice that we omitted the multinomial coefficient.  This is ok for classification because it is a constant w.r.t. $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log(p(x|y=e)): -7841.865447060635\n",
      "log(p(x|y=j)): -8771.433079075032\n",
      "log(p(x|y=s)): -8467.282044010557\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Function to compute the log-probability log(p(x|y)) for a given language and its log(theta) values\n",
    "def compute_log_probability(bow_vector, log_theta_values):\n",
    "    log_prob = 0  \n",
    "    for xi, char in zip(bow_vector, vocabulary):\n",
    "        log_prob += xi * log_theta_values[char]\n",
    "    return log_prob\n",
    "\n",
    "# Compute log(p(x|y)) for each language\n",
    "log_p_x_given_e = compute_log_probability(bow_vector, theta_e)\n",
    "log_p_x_given_j = compute_log_probability(bow_vector, theta_j)\n",
    "log_p_x_given_s = compute_log_probability(bow_vector, theta_s)\n",
    "\n",
    "print(\"log(p(x|y=e)):\", log_p_x_given_e)\n",
    "print(\"log(p(x|y=j)):\", log_p_x_given_j)\n",
    "print(\"log(p(x|y=s)):\", log_p_x_given_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(x|y=e): 0.0\n",
      "p(x|y=j): 0.0\n",
      "p(x|y=s): 0.0\n"
     ]
    }
   ],
   "source": [
    "# Convert the log probabilities back to regular probabilities\n",
    "p_x_given_e = math.exp(log_p_x_given_e)\n",
    "p_x_given_j = math.exp(log_p_x_given_j)\n",
    "p_x_given_s = math.exp(log_p_x_given_s)\n",
    "\n",
    "print(\"p(x|y=e):\", p_x_given_e)\n",
    "print(\"p(x|y=j):\", p_x_given_j)\n",
    "print(\"p(x|y=s):\", p_x_given_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Bayes rule and your estimated prior and likelihood, compute the posterior $\\hat p(y \\mid x)$.\n",
    "\n",
    "Show the three values: $\\hat p(y=e \\mid x), \\hat p(y=j \\mid x), \\hat p(y=s \\mid x)$.\n",
    "\n",
    "Show the predicted class label of $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(y=e|x): 1.0\n",
      "p(y=j|x): 0.0\n",
      "p(y=s|x): 2.4267389118368303e-272\n",
      "Predicted class label: e\n"
     ]
    }
   ],
   "source": [
    "# Compute log posterior for each language using the log priors\n",
    "log_posterior_e = log_p_x_given_e + p_e_smoothed\n",
    "log_posterior_j = log_p_x_given_j + p_j_smoothed\n",
    "log_posterior_s = log_p_x_given_s + p_s_smoothed\n",
    "\n",
    "# Normalize log posteriors by subtracting the max value among them\n",
    "max_log_posterior = max(log_posterior_e, log_posterior_j, log_posterior_s)\n",
    "\n",
    "# Convert normalized log posteriors back to regular probabilities\n",
    "posterior_e = math.exp(log_posterior_e - max_log_posterior)\n",
    "posterior_j = math.exp(log_posterior_j - max_log_posterior)\n",
    "posterior_s = math.exp(log_posterior_s - max_log_posterior)\n",
    "\n",
    "# Normalize the posteriors to sum to 1\n",
    "normalizing_factor = posterior_e + posterior_j + posterior_s\n",
    "normalized_posterior_e = posterior_e / normalizing_factor\n",
    "normalized_posterior_j = posterior_j / normalizing_factor\n",
    "normalized_posterior_s = posterior_s / normalizing_factor\n",
    "\n",
    "# Determine the predicted class label\n",
    "predicted_class_label = max([('e', normalized_posterior_e), ('j', normalized_posterior_j), ('s', normalized_posterior_s)], key=lambda x: x[1])[0]\n",
    "\n",
    "print(\"p(y=e|x):\", normalized_posterior_e)\n",
    "print(\"p(y=j|x):\", normalized_posterior_j)\n",
    "print(\"p(y=s|x):\", normalized_posterior_s)\n",
    "print(\"Predicted class label:\", predicted_class_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the performance of your classifier on the test set (files 10.txt to 19.txt in three languages).\n",
    "\n",
    "Present the performance using a confusion matrix. A confusion matrix summarizes the types of errors your classifier makes, as shown in the table below.   The columns are the true language a document is in, and the rows are the classified outcome of that document.  The cells are the number of test documents in that situation.  For example, the cell with row = English and column = Spanish contains the number of test documents that are really Spanish, but misclassified as English by your classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'e': {'e': 10, 'j': 0, 's': 0}, 'j': {'e': 0, 'j': 10, 's': 0}, 's': {'e': 0, 'j': 0, 's': 10}}\n"
     ]
    }
   ],
   "source": [
    "def predict_language(document):\n",
    "    bow_vector = [document.count(char) for char in vocabulary]\n",
    "    log_p_x_given_e = compute_log_probability(bow_vector, theta_e)\n",
    "    log_p_x_given_j = compute_log_probability(bow_vector, theta_j)\n",
    "    log_p_x_given_s = compute_log_probability(bow_vector, theta_s)\n",
    "    log_posterior_e = log_p_x_given_e + p_e_smoothed\n",
    "    log_posterior_j = log_p_x_given_j + p_j_smoothed\n",
    "    log_posterior_s = log_p_x_given_s + p_s_smoothed\n",
    "    predicted_class_label = max([('e', log_posterior_e), ('j', log_posterior_j), ('s', log_posterior_s)], key=lambda x: x[1])[0]\n",
    "    return predicted_class_label\n",
    "\n",
    "# Confusion matrix evaluation\n",
    "languages = ['e', 'j', 's']\n",
    "confusion_matrix = {lang1: {lang2: 0 for lang2 in languages} for lang1 in languages}\n",
    "\n",
    "base_path = \"languageID/\"  \n",
    "for lang in languages:\n",
    "    for i in range(10, 20):\n",
    "        filename = os.path.join(base_path, f\"{lang}{i}.txt\")\n",
    "        with open(filename, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "            document = file.read().lower()\n",
    "        predicted_label = predict_language(document)\n",
    "        confusion_matrix[predicted_label][lang] += 1  # Rows: Predicted, Columns: True\n",
    "\n",
    "print(confusion_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
